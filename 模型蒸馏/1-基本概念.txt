模型蒸馏的主要流程是先用完整复杂模型使用训练集训练出来一个teacher模型，然后设计一个小规模的student模型，再固定teacher模型的权重参数，然后设计一系列loss，让student模型在蒸馏学习的过程中逐渐向teacher模型的表现特性靠拢，使得student模型的预测精度逐渐逼近teacher模型。
其中，专门针对Bert模型的蒸馏方法有很多，如tinybert，distillBert,pkd-bert等等。虽然有这么多蒸馏方法，但是仔细研究也能发现它们或多或少都有一些共同点，例如：
1、在预训练阶段使用蒸馏方法通常能够取得较好的效果。
2、设计的loss都有一些共通性。
3、会将模型架构模块化，然后对模型不同的模块设计不同的loss。

对于知识蒸馏来说，本质是通过一种映射关系，将老师学到的东西映射到或者说传递给学生网络。

模型蒸馏，是希望能将用技巧将大模型中精华（暗知识）取出，注入到小模型中，从而使得小模型具备大模型的好性能。而通常蒸馏出的小模型，又要比直接用相同模型训练得到模型性能要好，这也是蒸馏意义所在。

将复杂的Teacher模型的结果蒸馏到相对简单的Student模型上面，Student模型学习Teacher模型预测的分布
