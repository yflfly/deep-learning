模型蒸馏的主要流程是先用完整复杂模型使用训练集训练出来一个teacher模型，然后设计一个小规模的student模型，再固定teacher模型的权重参数，然后设计一系列loss，让student模型在蒸馏学习的过程中逐渐向teacher模型的表现特性靠拢，使得student模型的预测精度逐渐逼近teacher模型。
其中，专门针对Bert模型的蒸馏方法有很多，如tinybert，distillBert,pkd-bert等等。虽然有这么多蒸馏方法，但是仔细研究也能发现它们或多或少都有一些共同点，例如：
1、在预训练阶段使用蒸馏方法通常能够取得较好的效果。
2、设计的loss都有一些共通性。
3、会将模型架构模块化，然后对模型不同的模块设计不同的loss。

对于知识蒸馏来说，本质是通过一种映射关系，将老师学到的东西映射到或者说传递给学生网络。

模型蒸馏，是希望能将用技巧将大模型中精华（暗知识）取出，注入到小模型中，从而使得小模型具备大模型的好性能。而通常蒸馏出的小模型，又要比直接用相同模型训练得到模型性能要好，这也是蒸馏意义所在。

将复杂的Teacher模型的结果蒸馏到相对简单的Student模型上面，Student模型学习Teacher模型预测的分布


知识蒸馏使用的是Teacher—Student模型，其中teacher是“知识”的输出者，student是“知识”的接受者。知识蒸馏的过程分为2个阶段:
原始模型训练: 训练"Teacher模型", 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对"Teacher模型"不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。
精简模型训练: 训练"Student模型", 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。