1、什么样的数据不适合用神经网络学习？
1)数据集太小，因为神经网络有效的关键就是大量的数据，有大量的参数需要训练，少量的数据不能充分训练参数。
2)数据集没有局部相关性。目前深度学习应用的领域主要是图像、语音、自然语言处理，这些领域的共性就是局部相关性。例如：图像中的像素组成物体，语音中的音位组成单词，文本数据中的单词组成句子，而深度学习的本质就是学习局部低层次的特征，然后组合低层次的特征成高层次的特征，得到不同特征之间的空间相关性。
####################################################################################################
2、什么是过拟合，什么是欠拟合？
过拟合：模型把数据学习的太彻底，以至于把噪声数据的特征也学习到了，这样就会导致在后期测试的时候不能够很好地识别数据，即不能正确的分类，模型泛化能力太差。
欠拟合：模型没有很好地捕捉到数据特征，不能够很好地拟合数据。
####################################################################################################
3、神经网络中的激活函数：对比ReLU与Sigmoid、Tanh的优缺点？
优点：
从计算的角度上，Sigmoid和Tanh激活函数均需要计算指数，复杂度高，而ReLU只需要一个阈值即可得到激活值；
ReLU的非饱和性可以有效地解决梯度消失的问题，提供相对宽的激活边界。
 ReLU的单侧抑制提供了网络的稀疏表达能力，和人脑的机制类似。
缺点：
ReLU和Sigmoid一样，它们的输出是非零中心化的，给后一层的神经网络引入偏置偏移， 会影响梯度下降的效率。
####################################################################################################
4、ReLU有哪些变种？
Leaky ReLU、ELU以及softplus函数。

relu(x) = max(0,x)

上述的ReLU对x小于0的情况均输出0，而LeakyReLU在x小于0时可以输出非0值
leakyrelu(x) = max(0,x)+negative_slope*min(0,x)

同样是针对ReLU的负数部分进行的改进，ELU激活函数对x小于零的情况采用类似指数计算的方式进行输出
elu(x) = max(0,x)+min(0,a*(exp(x)-1))
####################################################################################################
5、卷积神经网络相比于前馈神经网络，由哪些部分构成？各部分作用分别是什么？
如果用全连接前馈网络来处理图像时，会存在以下两个问题：
    参数太多；
    很难提取局部不变特征。
卷积神经网络一般是由卷积层、池化层和全连接层交叉堆叠而成的前馈神经网络，使用反向传播算法进行训练。
卷积层的作用：局部连接，权重共享；
池化层（pooling layer），也叫子采样层（subsampling layer）的作用：进行特征选择，降低特征数量，并从而减少参数数量。
####################################################################################################
6、在深度学习中，网络层数增多会伴随哪些问题，怎么解决？
1）计算资源的消耗（GPU）
2）模型容易过拟合（Dropout，正则化）
3）梯度消失/梯度爆炸问题的产生（批归一化BN、梯度裁剪、网络结构变更）：BN层能对各层的输出做归一化，这样梯度在反向层层传递后仍能保持分布稳定，不会出现过小或过大的情况。
4）degradation退化问题：在神经网络可以收敛的前提下，随着网络深度增加，随着网络层数的增多，训练集loss逐渐下降，然后趋于饱和，这时候再增加网络深度的话，训练集loss反而会增大。注意这并不是过拟合，因为在过拟合中训练loss是一直减小的。（ResNet的初衷，是让网络拥有恒等映射的能力，即能够在加深网络的时候，至少能保证深层网络的表现至少和浅层网络持平。如何做到恒等映射呢？事实上，神经网络很难拟合潜在的恒等映射函数H(x) = x，但如果把网络设计为H(x) = F(x) + x，即直接把恒等映射作为网络的一部分，就可以把问题转化为学习一个残差函数F(x) = H(x) - x, 而只要F(x) = 0，就构成了一个恒等映射。 ）
####################################################################################################
7、循环神经网络RNN中梯度消失与爆炸的原因是怎样的？
梯度消失：RNN梯度消失是因为激活函数tanh函数的倒数在0到1之间，反向传播时更新参数时，当参数W初始化为小于1的数，则多个(tanh函数’ * W)相乘，将导致求得的偏导极小（小于1的数连乘），从而导致远距离的项对于梯度的贡献很小，导致远距离的信息无法学习到。
梯度爆炸：当参数初始化为足够大，使得tanh函数的倒数乘以W大于1，则将导致偏导极大（大于1的数连乘），从而导致梯度爆炸。
####################################################################################################
8、循环神经网络RNN怎么解决长期依赖问题？
RNN中的长期依赖问题，也就是梯度消失或梯度爆炸可以采取如下方法解决：
    RNN梯度爆炸的解决方法：梯度截断
    RNN梯度消失的解决方法；残差结构、门控机制（LSTM、GRU）
为了RNN中的长期依赖问题，一种非常好的解决方案是引入门控Hochreiter and Schmidhuber 来控制信息的累积速度，包括有选择地加入新的信息，并有选择地遗忘之前累积的信息。这一类网络可以称为基于门控的循环神经网络（Gated RNN）。
####################################################################################################
9、什么是偏差，什么是方差？
偏差（bias）：
    度量了学习算法的期望预测与真实结果的偏离程度，即刻画了算法本身的拟合能力，偏差越大，表明越偏离真实值。
方差（variance）：
    度量了同样大小训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响，也就可以理解为衡量模型的稳定性（鲁棒性，泛化能力）。
粗略地说，偏差指的是算法在大型训练集上的错误率；方差指的是算法在测试集上的表现低于训练集的程度。
####################################################################################################
10、减小（可避免）偏差的方法有哪些？
加大模型规模（例如神经元/层的数量）
根据误差分析结果修改输入特征
减少或者去除正则化（L2 正则化，L1 正则化，dropout）
修改模型架构（比如神经网络架构）
####################################################################################################

未完待续